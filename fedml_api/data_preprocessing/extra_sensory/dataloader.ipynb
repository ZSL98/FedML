{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TRAIN_CLINETS_NUM = 60\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import io\n",
    "import torch\n",
    "\n",
    "def parse_header_of_csv(csv_str):\n",
    "    # Isolate the headline columns:\n",
    "    headline = csv_str[:csv_str.index(str.encode('\\n'))]\n",
    "    columns = headline.split(str.encode(','))\n",
    "\n",
    "    # The first column should be timestamp:\n",
    "    assert columns[0] == str.encode('timestamp')\n",
    "    # The last column should be label_source:\n",
    "    assert columns[-1] == str.encode('label_source')\n",
    "    \n",
    "    # Search for the column of the first label:\n",
    "    for (ci,col) in enumerate(columns):\n",
    "        if col.startswith(str.encode('label:')):\n",
    "            first_label_ind = ci\n",
    "            break\n",
    "        pass\n",
    "\n",
    "    # Feature columns come after timestamp and before the labels:\n",
    "    feature_names = columns[1:first_label_ind]\n",
    "    # Then come the labels, till the one-before-last column:\n",
    "    label_names = columns[first_label_ind:-1]\n",
    "    for (li,label) in enumerate(label_names):\n",
    "        # In the CSV the label names appear with prefix 'label:', but we don't need it after reading the data:\n",
    "        assert label.startswith(str.encode('label:'))\n",
    "        label_names[li] = label.replace(str.encode('label:'),str.encode(''))\n",
    "        pass\n",
    "    \n",
    "    return (feature_names,label_names)\n",
    "\n",
    "def parse_body_of_csv(csv_str,n_features):\n",
    "    # Read the entire CSV body into a single numeric matrix:\n",
    "    full_table = np.loadtxt(io.BytesIO(csv_str),delimiter=str.encode(','),skiprows=1)\n",
    "    \n",
    "    # Timestamp is the primary key for the records (examples):\n",
    "    timestamps = full_table[:,0].astype(int)\n",
    "    \n",
    "    # Read the sensor features:\n",
    "    X = full_table[:,1:(n_features+1)]\n",
    "    \n",
    "    # Read the binary label values, and the 'missing label' indicators:\n",
    "    trinary_labels_mat = full_table[:,(n_features+1):-1]; # This should have values of either 0., 1. or NaN\n",
    "    M = np.isnan(trinary_labels_mat); # M is the missing label matrix\n",
    "    Y = np.where(M,0,trinary_labels_mat) > 0.; # Y is the label matrix\n",
    "    \n",
    "    return (X,Y,M,timestamps)\n",
    "\n",
    "'''\n",
    "Read the data (precomputed sensor-features and labels) for a user.\n",
    "This function assumes the user's data file is present.\n",
    "'''\n",
    "def read_user_data(uuid, data_path='../../../data/extra_sensory/ExtraSensory.per_uuid_features_labels/'):\n",
    "    user_data_file = '%s' % uuid\n",
    "    # Read the entire csv file of the user:\n",
    "    with gzip.open(data_path + user_data_file,'rb') as fid:\n",
    "        csv_str = fid.read()\n",
    "        pass\n",
    "\n",
    "    (feature_names,label_names) = parse_header_of_csv(csv_str)\n",
    "    n_features = len(feature_names)\n",
    "    (X,Y,M,timestamps) = parse_body_of_csv(csv_str,n_features)\n",
    "\n",
    "    return (X,Y,M,timestamps,feature_names,label_names)"
   ]
  },
  {
   "source": [
    "def get_sensor_names_from_features(feature_names):\n",
    "    feat_sensor_names = np.array([None for feat in feature_names])\n",
    "    for (fi,feat) in enumerate(feature_names):\n",
    "        if feat.startswith(b'raw_acc'):\n",
    "            feat_sensor_names[fi] = 'Acc'\n",
    "            pass\n",
    "        elif feat.startswith(b'proc_gyro'):\n",
    "            feat_sensor_names[fi] = 'Gyro'\n",
    "            pass\n",
    "        elif feat.startswith(b'raw_magnet'):\n",
    "            feat_sensor_names[fi] = 'Magnet'\n",
    "            pass\n",
    "        elif feat.startswith(b'watch_acceleration'):\n",
    "            feat_sensor_names[fi] = 'WAcc'\n",
    "            pass\n",
    "        elif feat.startswith(b'watch_heading'):\n",
    "            feat_sensor_names[fi] = 'Compass'\n",
    "            pass\n",
    "        elif feat.startswith(b'location'):\n",
    "            feat_sensor_names[fi] = 'Loc'\n",
    "            pass\n",
    "        elif feat.startswith(b'location_quick_features'):\n",
    "            feat_sensor_names[fi] = 'Loc'\n",
    "            pass\n",
    "        elif feat.startswith(b'audio_naive'):\n",
    "            feat_sensor_names[fi] = 'Aud'\n",
    "            pass\n",
    "        elif feat.startswith(b'audio_properties'):\n",
    "            feat_sensor_names[fi] = 'AP'\n",
    "            pass\n",
    "        elif feat.startswith(b'discrete'):\n",
    "            feat_sensor_names[fi] = 'PS'\n",
    "            pass\n",
    "        elif feat.startswith(b'lf_measurements'):\n",
    "            feat_sensor_names[fi] = 'LF'\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"!!! Unsupported feature name: %s\" % feat)\n",
    "        pass\n",
    "    return feat_sensor_names;    \n",
    "\n",
    "def project_features_to_selected_sensors(X,feat_sensor_names,sensors_to_use):\n",
    "    use_feature = np.zeros(len(feat_sensor_names),dtype=bool)\n",
    "    for sensor in sensors_to_use:\n",
    "        is_from_sensor = (feat_sensor_names == sensor)\n",
    "        use_feature = np.logical_or(use_feature,is_from_sensor)\n",
    "        pass\n",
    "    X = X[:,use_feature]\n",
    "    return X\n",
    "\n",
    "def estimate_standardization_params(X_train):\n",
    "    mean_vec = np.nanmean(X_train,axis=0)\n",
    "    std_vec = np.nanstd(X_train,axis=0)\n",
    "    return (mean_vec,std_vec)\n",
    "\n",
    "def standardize_features(X,mean_vec,std_vec):\n",
    "    # Subtract the mean, to centralize all features around zero:\n",
    "    X_centralized = X - mean_vec.reshape((1,-1))\n",
    "    # Divide by the standard deviation, to get unit-variance for all features:\n",
    "    # * Avoid dividing by zero, in case some feature had estimate of zero variance\n",
    "    normalizers = np.where(std_vec > 0., std_vec, 1.).reshape((1,-1))\n",
    "    X_standard = X_centralized / normalizers\n",
    "    return X_standard"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_extra_sensory(X, Y, M, timestamps, feature_names, label_names): \n",
    "    X_all = X.tolist()\n",
    "    Y_all = Y.tolist()\n",
    "    M_all = M.tolist()\n",
    "    T_all = timestamps.tolist()\n",
    "    for i in range(len(X)-1, -1, -1):\n",
    "        labels_to_display = [b'LYING_DOWN',b'SITTING',b'OR_standing',b'FIX_walking',b'FIX_running']\n",
    "        Y_all[i] = [Y[i][label_names.index(label)] for label in labels_to_display]\n",
    "        if np.sum(Y_all[i]) != 1:\n",
    "            Y_all.pop(i)\n",
    "            T_all.pop(i)\n",
    "            X_all.pop(i)\n",
    "            M_all.pop(i)\n",
    "        else:\n",
    "            T_all[i] = T_all[i] - T_all[0]\n",
    "            Y_all[i] = Y_all[i].index(1)\n",
    "    sensors_to_use = ['Acc','WAcc']\n",
    "    feat_sensor_names = get_sensor_names_from_features(feature_names)\n",
    "    X_all = project_features_to_selected_sensors(np.array(X_all),feat_sensor_names,sensors_to_use)\n",
    "    (mean_vec,std_vec) = estimate_standardization_params(X_all)\n",
    "    X_all = standardize_features(X_all, mean_vec, std_vec)\n",
    "\n",
    "    return X_all, Y_all, M_all, T_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_x, data_y, batch_size):\n",
    "    '''\n",
    "    data is a dict := {'x': [numpy array], 'y': [numpy array]} (on one client)\n",
    "    returns x, y, which are both numpy array of length: batch_size\n",
    "    '''\n",
    "    # randomly shuffle data\n",
    "    np.random.seed(100)\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(data_x)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(data_y)\n",
    "\n",
    "    # loop through mini-batches\n",
    "    batch_data = list()\n",
    "    for i in range(0, len(data_x), batch_size):\n",
    "        batched_x = data_x[i:i + batch_size]\n",
    "        batched_y = data_y[i:i + batch_size]\n",
    "        batched_x = torch.from_numpy(np.asarray(batched_x)).float()\n",
    "        batched_y = torch.from_numpy(np.asarray(batched_y)).long()\n",
    "        batch_data.append((batched_x, batched_y))\n",
    "    return batch_data"
   ]
  },
  {
   "source": [
    "def read_all_data(data_path='../../../data/extra_sensory/ExtraSensory.per_uuid_features_labels/'):\n",
    "    files = os.listdir(data_path)\n",
    "    client_idx = 0\n",
    "    train_data_local_dict = dict()\n",
    "    test_data_local_dict = dict()\n",
    "    for user_data_file in files:\n",
    "        if user_data_file.endswith('.csv.gz'):\n",
    "            (X,Y,M,timestamps,feature_names,label_names) = read_user_data(user_data_file)\n",
    "            X_all, Y_all, M_all, T_all = preprocess_extra_sensory(X, Y, M, timestamps, feature_names, label_names)\n",
    "            #train_data_local_num_dict[client_idx] = user_train_data_num\n",
    "            #train_data_local_num_dict[client_idx] = user_train_data_num\n",
    "            #train_data_num += len(X)\n",
    "            #test_data_num = train_data_num\n",
    "            # TODO: train_data_num and test_data_num are for what?\n",
    "            time_horizon = len(X) - 500\n",
    "            batch_size = 10\n",
    "            train_X = X_all[:time_horizon]\n",
    "            test_X = X_all[time_horizon+1:]\n",
    "            train_Y = Y_all[:time_horizon]\n",
    "            test_Y = Y_all[time_horizon+1:]\n",
    "            train_batch = batch_data(train_X, train_Y, batch_size)\n",
    "            test_batch = batch_data(test_X, test_Y, batch_size)\n",
    "            train_data_local_dict[client_idx] = train_batch\n",
    "            test_data_local_dict[client_idx] = test_batch\n",
    "            client_idx += 1\n",
    "        else:\n",
    "            continue\n",
    "    client_num = client_idx\n",
    "    class_num = 5\n",
    "    return client_num, train_data_local_dict, test_data_local_dict, class_num\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client_num, train_data_local_dict, test_data_local_dict, class_num = read_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(-0.2315)\ntensor(-0.3048)\ntensor(-0.3213)\ntensor(0.4856)\ntensor(1.1146)\ntensor(-0.6516)\ntensor(0.5082)\ntensor(-0.4835)\ntensor(0.1988)\ntensor(0.4583)\ntensor(0.4714)\ntensor(0.6998)\ntensor(-0.1428)\ntensor(0.3075)\ntensor(-0.1870)\ntensor(0.4136)\ntensor(-0.4594)\ntensor(-0.1748)\ntensor(5.4908)\ntensor(-0.4892)\ntensor(0.4994)\ntensor(0.1399)\ntensor(-0.4133)\ntensor(-0.2601)\ntensor(-0.2385)\ntensor(-0.1786)\ntensor(3.9188)\ntensor(0.1549)\ntensor(-0.2705)\ntensor(-0.4013)\ntensor(0.2465)\ntensor(0.2468)\ntensor(-0.6395)\ntensor(-0.5797)\ntensor(0.4947)\ntensor(-0.2864)\ntensor(-0.4694)\ntensor(-0.6260)\ntensor(0.4928)\ntensor(-0.1952)\ntensor(-0.3952)\ntensor(-0.1277)\ntensor(-0.1114)\ntensor(0.2086)\ntensor(0.0035)\ntensor(2.6951)\ntensor(-0.2566)\ntensor(-0.1527)\ntensor(-0.1178)\ntensor(-0.1988)\ntensor(-0.3131)\ntensor(0.4940)\ntensor(-0.3518)\ntensor(0.2127)\ntensor(-0.2532)\ntensor(-0.5803)\ntensor(-0.4109)\ntensor(0.4925)\ntensor(0.3526)\ntensor(0.1524)\ntensor(-0.4565)\ntensor(0.2061)\ntensor(0.4861)\ntensor(-0.1224)\ntensor(-0.4570)\ntensor(4.8253)\ntensor(-0.3485)\ntensor(-0.1532)\ntensor(-0.3876)\ntensor(-0.2488)\ntensor(0.4018)\ntensor(-0.5676)\ntensor(-0.5465)\ntensor(-0.2398)\ntensor(0.4861)\ntensor(3.9213)\ntensor(-0.7641)\ntensor(-0.2649)\ntensor(-0.4853)\ntensor(-2.0825)\ntensor(-0.1406)\ntensor(1.1480)\ntensor(-0.2712)\ntensor(-0.1379)\ntensor(-0.1984)\ntensor(-0.6176)\ntensor(1.1447)\ntensor(-0.3746)\ntensor(-0.1923)\ntensor(0.5172)\ntensor(-0.5487)\ntensor(-2.3630)\ntensor(0.4772)\ntensor(-0.5007)\ntensor(-0.1390)\ntensor(-0.4055)\ntensor(-0.6938)\ntensor(-0.1769)\ntensor(-0.1362)\ntensor(0.2886)\ntensor(-0.2095)\ntensor(-0.5963)\ntensor(0.3567)\ntensor(-0.2187)\ntensor(-0.1969)\ntensor(-0.1920)\ntensor(-0.2187)\ntensor(-0.4103)\ntensor(-0.5642)\ntensor(-0.8754)\ntensor(-0.8043)\ntensor(-0.1921)\ntensor(-0.5949)\ntensor(-0.6254)\ntensor(0.2150)\ntensor(0.5132)\ntensor(-0.5456)\ntensor(0.2328)\ntensor(0.4755)\ntensor(-0.2475)\ntensor(2.5111)\ntensor(-0.2000)\ntensor(0.6866)\ntensor(0.4646)\ntensor(-0.5839)\ntensor(0.1877)\ntensor(-0.3080)\ntensor(-0.2676)\ntensor(0.1830)\ntensor(-0.1642)\ntensor(2.4584)\ntensor(1.7311)\ntensor(-0.1896)\ntensor(-0.1532)\ntensor(-0.1621)\ntensor(-0.2775)\ntensor(0.9411)\ntensor(0.1603)\ntensor(0.4864)\ntensor(-0.1383)\ntensor(-0.1997)\ntensor(-0.1864)\ntensor(-0.5681)\ntensor(5.8580)\ntensor(-0.2212)\ntensor(-0.5783)\ntensor(-0.5357)\ntensor(-0.5185)\ntensor(-0.4549)\ntensor(-0.3822)\ntensor(-0.5053)\ntensor(-0.3882)\ntensor(0.2084)\ntensor(0.4101)\ntensor(-0.4470)\ntensor(0.6804)\ntensor(-0.2486)\ntensor(0.1960)\ntensor(0.1400)\ntensor(0.2964)\ntensor(-0.4963)\ntensor(-0.2360)\ntensor(0.5606)\ntensor(-2.4271)\ntensor(-0.2456)\ntensor(-0.2205)\ntensor(-0.2953)\ntensor(-0.2079)\ntensor(3.5142)\ntensor(0.2274)\ntensor(-0.4599)\ntensor(-0.1735)\ntensor(-0.2972)\ntensor(0.5322)\ntensor(-0.6052)\ntensor(-0.4719)\ntensor(0.7976)\ntensor(0.1735)\ntensor(-0.2055)\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (x, labels) in enumerate(train_data_local_dict[0]):\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}